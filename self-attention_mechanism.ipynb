{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d81e69",
   "metadata": {},
   "source": [
    "## Steps for a simple self-attention mechanism\n",
    "1. Get input vectors for each token\n",
    "2. Get the dot product of your specific token, this is called the attention score\n",
    "3. Normalize the weights from your attention score using softmax(or a different normalization technique)\n",
    "4. Get the context vector of your specific token by using scalar multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa5375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1ee588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vectors for \"Your journey starts with one step\"\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts (x^3)\n",
    "    [0.22, 0.58, 0.33], # with (x^4)\n",
    "    [0.77, 0.25, 0.10], # one (x^5)\n",
    "    [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b5fff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37021b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c20309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3910e+10, 4.2117e-41, 1.3910e+10, 4.2117e-41, 1.3910e+10, 4.2117e-41])\n"
     ]
    }
   ],
   "source": [
    "print(torch.empty(inputs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b68d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product of \"Your\" and \"Journey\": 0.9544000625610352\n"
     ]
    }
   ],
   "source": [
    "# Sample dot product of \"Your\" and \"Journey\"\n",
    "# .43 * .55 + .15 * .87 + .89 * .66 = 0.2365 + 0.1305 + 0.5874 = 0.9544\n",
    "\n",
    "dot_product = torch.dot(inputs[0], inputs[1])\n",
    "print(f'Dot Product of \"Your\" and \"Journey\": {dot_product.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152005c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Generating attention score for inputs[1] which is \"journey\"\n",
    "# To generate an attention score for a given query vector, we compute the dot product\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6db15164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor(0.9544)\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor(1.4950)\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor(1.4754)\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor(0.8434)\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor(0.7070)\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor(1.0865)\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(i, x_i)\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "    print(attn_scores_2[i])\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa7ec70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights using Linear Normalization: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize weights for \"journey\"\n",
    "# This type of normalization is called \"Linear Normalization\" or \"Min-max Scaling\"\n",
    "# Each token has an attention weight that sums to 1\n",
    "# The attention weight vector has the same length as the number of tokens\n",
    "# Each number in the vector corresponds to the attention weight for that token, so the first number is the attention weight for \"Your\", the second for \"journey\", and so on\n",
    "# Linear Normalization is not preferred because it does not handle negative scores well and can lead to skewed distributions\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights using Linear Normalization:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774dd10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981a7b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights using naive softmax: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Softmax Normalization for \"journey\"\n",
    "# Softmax is used to convert raw attention scores into probabilities that sum to 1\n",
    "# This is better than Linear Normalization because:\n",
    "#  it emphasizes the highest scores more strongly\n",
    "#  it ensures all weights are positive\n",
    "#  it provides a smoother gradient\n",
    "\n",
    "# Softmax function\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights using naive softmax:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1ad35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights from PyTorch's softmax: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Using PyTorch's built-in softmax function which is optimized and numerically stable\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights from PyTorch's softmax:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b082c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.13854756951332092   tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.0596, 0.0208, 0.1233])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.2378913015127182   tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.1904, 0.2277, 0.2803])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.23327402770519257   tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.3234, 0.4260, 0.4296])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.12399158626794815   tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.3507, 0.4979, 0.4705])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.10818186402320862   tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.4340, 0.5250, 0.4813])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.15811361372470856   tensor([0.0500, 0.8000, 0.5500])\n",
      "Context vector for 'journey': tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Generating context vector for \"journey\"\n",
    "# The context vector is a weighted sum of the input vectors, where the weights are the attention weights\n",
    "# Context Vector = Î£ (attention_weight_i * input_vector_i) which is a scalar-vector multiplication followed by a summation\n",
    "# scalar multiplication = attention_weight_i * input_vector_i\n",
    "# summation = sum(scalar multiplication)\n",
    "\n",
    "query = inputs[1]\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i,x_i in enumerate(inputs):\n",
    "    print(context_vec_2)\n",
    "\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(f\"{attn_weights_2[i]}   {x_i}\")\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(\"Context vector for 'journey':\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76b9c9",
   "metadata": {},
   "source": [
    "### Now that we've gone through the steps for the token \"journey\", this is how you compute the whole thing at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca691de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate attention scores for all queries\n",
    "\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6863bb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd34ca14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
       "        [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
       "        [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f148169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can compute all attention scores at once using matrix multiplication\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7daa233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac819fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Get attention weights for all queries using softmax, all rows should sum up to 1\n",
    "# dim=-1 indicates that softmax is applied across the last dimension (i.e., across each row)\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "658aa89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Generating context vectors for all queries at once\n",
    "\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8d70997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "andy = torch.matmul(attn_weights, inputs)\n",
    "andy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a368b5",
   "metadata": {},
   "source": [
    "## Now we move on to making the self-attention mechanism trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3847f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output dimensions, again we're using inputs[1] which is \"journey\" as our example\n",
    "\n",
    "# x_2 is the input vector for \"journey\"\n",
    "x_2 = inputs[1]\n",
    "\n",
    "# The dimension of \"journey\" which is 3\n",
    "d_in = inputs.shape[1]\n",
    "\n",
    "# Why do we set the dimension to 2 instead of 3?\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b98b69bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a6b2ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize random weight matrices[3,2] for Query, Key, and Value\n",
    "# requires_grad=False to indicate these are fixed for this example, if we were training a model, we would set requires_grad=True\n",
    "# Since we set the seed manually, we will get the same random numbers every time for our weights\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Why are weights randomized?\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# @ symbol denotes matrix multiplication in PyTorch\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8f89338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e109199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff2fbe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e526d94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4433, 1.1419])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d530394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00d9d22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "801ede30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfaf0c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4361, 1.1156],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1827, 0.3292],\n",
       "        [0.3275, 0.9642]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate keys and values for all inputs at once\n",
    "\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ca60793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n",
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# Get attention score for query_2 against all keys\n",
    "\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n",
    "\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0010722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "The attention weights are: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we have to normalize these attention scores to get attention weights\n",
    "# We will use softmax normalization\n",
    "\n",
    "# d_k is the dimension of the key vectors\n",
    "d_k = keys.shape[1]\n",
    "\n",
    "print(d_k)\n",
    "\n",
    "# \n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(\"The attention weights are:\",attn_weights_2)\n",
    "\n",
    "attn_weights_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "673e264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context vector is: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# We now compute the context vector for query_2 using the attention weights and the value vectors\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"The context vector is:\",context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f860a67",
   "metadata": {},
   "source": [
    "## Implementing a trainable self-attention but with every token at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5e4be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from SelfAttention_v1: tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = inputs @ W_key\n",
    "        values = inputs @ W_value\n",
    "        queries = inputs @ W_query\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in=3, d_out=2)\n",
    "output_v1 = sa_v1(inputs)\n",
    "\n",
    "# We can double check if our self attention works by comparing output_v1[1] with context_vec_2 \"journey\"\n",
    "print(\"Output from SelfAttention_v1:\", output_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fdfa30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ef7b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from SelfAttention_v2: tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Improved Self-Attention with Linear Layers\n",
    "# It's better to use nn.Linear layers for the weight matrices because they handle biases and initialization more effectively\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in=3, d_out=2, qkv_bias=False)\n",
    "output_v2 = sa_v2(inputs)\n",
    "print(\"Output from SelfAttention_v2:\", output_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f5736",
   "metadata": {},
   "source": [
    "## Causal attention\n",
    "\n",
    "### Also known as masked attention is used to \"mask\" the next tokens. It is used for a more \"creative\" output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da190285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights from reused weights:\n",
      " tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We are going to reuse the query and key weight from SelfAttention_v2 for convenience\n",
    "\n",
    "# queries and keys is a weight matrix\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[1]**0.5, dim=-1)\n",
    "print(\"Attention weights from reused weights:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a47fb56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "We have a diagonal of zeros which are future tokens that should not be attended to.\n"
     ]
    }
   ],
   "source": [
    "# We will now make a \"mask\" using `tril` to prevent attention to future tokens\n",
    "\n",
    "context_length = attn_weights.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n",
    "print(\"We have a diagonal of zeros which are future tokens that should not be attended to.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "caf63e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention weights:\n",
      " tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# I don't get this part\n",
    "\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(\"Masked attention weights:\\n\", masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa44b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921],\n",
      "        [0.3700],\n",
      "        [0.5357],\n",
      "        [0.6775],\n",
      "        [0.8415],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "Masked and normalized attention weights:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Each row sums to one: tensor([1., 1., 1., 1., 1., 1.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Why is keepdim=True here? What does it do?\n",
    "# dim=-1 means that the summation is done across the last dimension (i.e., across each row)\n",
    "\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "print(row_sums)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(\"Masked and normalized attention weights:\\n\", masked_simple_norm)\n",
    "print(\"Each row sums to one:\", masked_simple_norm.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2511a5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention scores with -inf:\n",
      " tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# When negative infinity are present, softmax treats them as zeros\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Masked attention scores with -inf:\\n\", masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fd3c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention weights after softmax:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to get masked attention weights\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[1]**0.5, dim=-1)\n",
    "print(\"Masked attention weights after softmax:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8c56a",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "### is a technique where randomly selected hidden layer units are ignored during training (these layers are \"dropped out\")\n",
    "We do this technique to help prevent overfitting by ensuring that the model does not become reliant on any specific layer.\n",
    "Dropout is ONLY used during training and is disabled afterwards.\n",
    "Dropout is typically applied at 2 times:\n",
    "1. After calculating attention weights\n",
    "2. After applying the attention weights to the value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "785e304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example after applying dropout:\n",
      " tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# We will use a high dropout rate of 50% (which means masking half of the attention weights) to demonstrate the effect\n",
    "\n",
    "# Dropout example\n",
    "torch.manual_seed(123)\n",
    "dropout_rate = 0.5\n",
    "dropout = torch.nn.Dropout(dropout_rate)\n",
    "example = torch.ones(6, 6)\n",
    "dropped = dropout(example)\n",
    "print(\"Example after applying dropout:\\n\", dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82e7405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6380, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5090, 0.5085, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4120, 0.0000, 0.3869, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3418, 0.3413, 0.3308, 0.3249, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69484c",
   "metadata": {},
   "source": [
    "## Implementing a compact causal attention to use for the multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45e61e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "\n",
    "# Batch shape is [2,6,3] because two input texts with six tokens each where each token is a three-dimensional embedding vector\n",
    "print(\"Batch shape:\", batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1591b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "        dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "        diagonal=1)\n",
    "    )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d403dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc784d",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "### We will be dividing the attention mechanism into multiple \"heads\" which each operate independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(\n",
    "                 d_in, d_out, context_length, dropout, qkv_bias\n",
    "             ) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a91b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads   \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)        \n",
    "        queries = self.W_query(x)   \n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim                    \n",
    "        )                                                                   \n",
    "        keys = keys.transpose(1, 2)         \n",
    "        queries = queries.transpose(1, 2)   \n",
    "        values = values.transpose(1, 2)     \n",
    "        attn_scores = queries @ keys.transpose(2, 3)  \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]   \n",
    "  \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)    \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  \n",
    "        \n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)   \n",
    "        return context_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
